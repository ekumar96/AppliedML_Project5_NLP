# Project 5 - Spring 2022

Applied ML Spring 2022 Project 5: NLP, Bag of Words and TF-IDF
Eshan Kumar, ek3227

This project consists of one part.

# Part 1 - Comparing BOW with TF-IDF









In the first part of the project, we define a neural network and write functions to do forward propogation and backpropogation on this network from scratch.

### Forward Propogation
First, weights and biases are initialized randomly, and the activation function (sigmoid) is defined. Forward propogation is simply the value of the nodes, multiplied by the weights, and then plus the biases. Then the activation function is applied and the process is repeated. 

### Backward Propogation
The backward pass is done by first comparing the Y values generated by the network to the ground truth Y values. The loss (MSE Loss) is calculated, and the gradient with respect to Y (dLdY) is found. Then, the inverse of the activation is applied to find dLdZ2. dLdW2 and dLdb2 are found by linear algebraic operations on the hidden layers and dLdZ2. This process is then repeated for the previous layer's weights and biases. 

# Part 2/3 - Neural Networks and the Fashion MNIST dataset
In this part, we use train a neural network with images of clothes and their respective labels from the Fashion MNIST dataset. 

The Fashion-MNIST dataset is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It's commonly used as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning models. You can read more about the dataset at the Fashion-MNIST homepage.

## Data Preprocessing
To prepare the data for the neural network, we first flatten the images into 1d vectors, then standardize them (constrain the pixel values from RGB 0-255 to 0-1). We then use one hot encoding on the label (the categories of clothes), and split the data into train, validation, and test sets. 

## Model Creation
We create the neural network model using Softmax activation for the last layer and ReLU activation for every other layer. We have an initial hidden layer of size 128, followed by a hidden layer of size 64, and finally a last layer of size 10 (the number of categories of clothing). We print out a model summary to evaluate how the model looks and the number of parameters, then try to insert one data point into the neural network to ensure that it was created with the correct architecture. 

## Model Training and Evaluation
The model is trained for 20 epochs with a batch size of 128, using Adam as the optimizer, and the values of loss function and metric after each epoch for both train and validation sets are recorded. This is then plotted in two separate plots displaying train vs validation loss and train vs validation metric scores over each epoch. The trained model is then used on the test set to evaluate performance and accuracy. Next, I plot a confusion matrix on the test set and label the axes appropriately with true and predicted labels. Finally, I visualize the same dataset as earlier, but this time with true and predicted labels from the model. It seems that the model is least accurate at predicting hoodies. 

# Part 4 - Convolutional Neural Network
In this part, I build and train a classical convolutional neural network, LeNet-5, on the Fashion-MNIST dataset.

## Data Preprocessing
Simlarly to above, I reload and prepare the data for the CNN. I do not however, flatten the images into 1d vectors, as this is not required for CNNs, which move a kernel/filter over the image. I standardize them (constrain the pixel values from RGB 0-255 to 0-1). We then use one hot encoding on the label (the categories of clothes), and split the data into train, validation, and test sets. 

## Model Creation
I build the LeNet-5 CNN as described by Yann LeCunn, then visualize the neural network and ensure that it appears correct.

## Model Training and Evaluation
The model is trained for 20 epochs with a batch size of 128, using Adam as the optimizer, and the values of loss function and metric after each epoch for both train and validation sets are recorded. This is then plotted in two separate plots displaying train vs validation loss and train vs validation metric scores over each epoch. The trained model is then used on the test set to evaluate performance and accuracy. We see that near the end, even though training accuracy is increasing and training loss is decreasing, validation loss and accuracy are stagnant, or even increasing. This is a good sign of overfitting. 

## Overfitting prevention
To overcome overfitting, I train the network using dropout layers, then I try batch normalization.

### Dropout
I again build a CNN, but with dropout this time with dropout probability of 0.5. I train the model again for 15 epochs, use two plots to display train vs validation metric scores and train vs validation loss over each epoch. Then I report model performance on the test set, and find  the Validation Loss seems to be steadily decreasing, and matches along well with the train loss. This is a good indication that the model is learning properly, and may even benefit from additional epochs. The accuracy also seems to be slightly improving for both training and validation data. Dropout simulates ensembling, so it makes sense that this would help prevent overfitting.

### Batch Normalization
Next, I again built a CNN, but with a batch normalization after every hidden layer. I train the model for 15 epochs, plot the metric scores and loss values, and report model performance on test set as above. Upon comparison with dropout, I find that dropout is more effective, and that even with batch normalization, the model performs worse on the test set than the baseline, and also worse than dropout. The validation loss is not consistently decreasing, and the validation accuracy is staying constant while train accuracy is increasing and train loss decreases. This is a good indicator that our model is overfitting.
